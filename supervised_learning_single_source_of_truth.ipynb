{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff178805",
   "metadata": {},
   "source": [
    "# Supervised Learning â€” Single Source of Truth\n",
    "\n",
    "**Contents:** Naive Bayes, Performance Evaluation, Naive Bayes Optimizations, K-Nearest Neighbors (KNN), Decision Trees, Linear Regression, Logistic Regression, Support Vector Machine (SVM).\n",
    "\n",
    "This notebook provides:\n",
    "\n",
    "- Detailed definitions and theory (technical overview) with mathematical formulas rendered in LaTeX.\n",
    "- Plain-English intuitive explanations.\n",
    "- Key terms, strengths, and limitations.\n",
    "- Runnable Python examples for each algorithm with line-by-line comments.\n",
    "\n",
    "---\n",
    "\n",
    "_Generated as a single comprehensive handbook for interview-ready and reference use._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee5dd96",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Naive Bayes](#Naive-Bayes)\n",
    "2. [Performance Evaluation](#Performance-Evaluation)\n",
    "3. [Naive Bayes Optimizations](#Naive-Bayes-Optimizations)\n",
    "4. [K-Nearest Neighbors (KNN)](#K-Nearest-Neighbors)\n",
    "5. [Decision Trees](#Decision-Trees)\n",
    "6. [Linear Regression](#Linear-Regression)\n",
    "7. [Logistic Regression](#Logistic-Regression)\n",
    "8. [Support Vector Machine (SVM)](#Support-Vector-Machine)\n",
    "9. [Conclusion & References](#Conclusion-and-References)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd2455",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "### Definition & Working Principle\n",
    "\n",
    "Naive Bayes is a probabilistic classifier based on **Bayes' Theorem**, which computes the posterior probability of a class given observed features. The core assumption is that features are **conditionally independent** given the class.\n",
    "\n",
    "### Bayes' Theorem (basic formula)\n",
    "\n",
    "\\[\n",
    "P(C_k \\mid X) = \\frac{P(X \\mid C_k)\\,P(C_k)}{P(X)}\n",
    "\\]\n",
    "\n",
    "Under the Naive assumption (features independent):\n",
    "\n",
    "\\[\n",
    "P(X \\mid C_k) = \\prod_{i=1}^{n} P(x_i \\mid C_k)\n",
    "\\]\n",
    "\n",
    "### Likelihoods / Variants\n",
    "\n",
    "- **Gaussian Naive Bayes:** assumes continuous features follow a normal distribution. For each feature:\n",
    "\\[ P(x_i \\mid C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_{k,i}^2}} \\exp\\left(-\\frac{(x_i-\\mu_{k,i})^2}{2\\sigma_{k,i}^2}\\right) \\]\n",
    "\n",
    "- **Multinomial Naive Bayes:** models counts (e.g., word counts) with likelihood proportional to relative frequency.\n",
    "\n",
    "- **Bernoulli Naive Bayes:** models binary/bag-of-words presence/absence.\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "- **Prior**: \\(P(C_k)\\), the prevalence of class before seeing data.\n",
    "- **Likelihood**: \\(P(X\\mid C_k)\\), how probable the features are under class \\(C_k\\).\n",
    "- **Posterior**: \\(P(C_k\\mid X)\\), the updated probability after observing \\(X\\).\n",
    "- **Laplace smoothing**: add \\(\\alpha\\) to counts to avoid zero probabilities: \\(P=\\frac{count+\\alpha}{N+\\alpha|V|}\\).\n",
    "\n",
    "### Strengths & Limitations\n",
    "\n",
    "- Strengths: extremely fast to train and predict, works well on high-dimensional problems (text), low memory footprint.\n",
    "- Limitations: strong independence assumption, does not model feature interactions; can be outperformed when features are highly correlated.\n",
    "\n",
    "### Plain-English Summary\n",
    "\n",
    "Naive Bayes treats each feature as an independent witness and multiplies their evidences to find the most likely class. It's simple but often surprisingly effective, especially in text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0799c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes -- Gaussian and Multinomial examples with comments on each line\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Example 1: Gaussian Naive Bayes with Iris dataset (continuous features)\n",
    "X, y = load_iris(return_X_y=True)  # load a small continuous dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  # split data\n",
    "gnb = GaussianNB()  # instantiate Gaussian Naive Bayes\n",
    "gnb.fit(X_train, y_train)  # fit model to training data\n",
    "y_pred = gnb.predict(X_test)  # predict on test set\n",
    "print('GaussianNB classification report:')  # label\n",
    "print(classification_report(y_test, y_pred))  # show precision/recall/f1\n",
    "\n",
    "# Example 2: Multinomial Naive Bayes for text (counts)\n",
    "texts = ['spam offer buy now', 'limited offer click', 'hello how are you', 'let us meet tomorrow']  # tiny corpus\n",
    "labels = [1, 1, 0, 0]  # binary labels: 1=spam, 0=ham\n",
    "vec = CountVectorizer()  # convert text to token counts\n",
    "X_counts = vec.fit_transform(texts)  # fit and transform\n",
    "mnb = MultinomialNB(alpha=1.0)  # Laplace smoothing alpha=1\n",
    "mnb.fit(X_counts, labels)  # train MultinomialNB\n",
    "sample = vec.transform(['buy this limited offer now'])  # new sample to classify\n",
    "print('MultinomialNB prediction for sample:', mnb.predict(sample))  # output predicted class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a4dca0",
   "metadata": {},
   "source": [
    "## Performance Evaluation\n",
    "\n",
    "### Definition & Purpose\n",
    "\n",
    "Performance evaluation quantifies how well a supervised model will perform on unseen data. It is crucial for model selection, hyperparameter tuning, and deployment decisions.\n",
    "\n",
    "### Classification Metrics (confusion matrix components)\n",
    "\n",
    "- True Positive (TP), True Negative (TN), False Positive (FP), False Negative (FN)\n",
    "\n",
    "Formulas:\n",
    "\n",
    "Accuracy:\n",
    "\n",
    "\\[ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} \\]\n",
    "\n",
    "Precision:\n",
    "\n",
    "\\[ \\text{Precision} = \\frac{TP}{TP + FP} \\]\n",
    "\n",
    "Recall (Sensitivity):\n",
    "\n",
    "\\[ \\text{Recall} = \\frac{TP}{TP + FN} \\]\n",
    "\n",
    "F1-score:\n",
    "\n",
    "\\[ F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\]\n",
    "\n",
    "### Regression Metrics\n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "\n",
    "\\[ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\]\n",
    "\n",
    "R-squared (coefficient of determination):\n",
    "\n",
    "\\[ R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2} \\]\n",
    "\n",
    "### Validation Strategies\n",
    "\n",
    "- Train/test split\n",
    "- k-fold cross-validation\n",
    "- Stratified k-fold for imbalanced classes\n",
    "\n",
    "### Practical Notes\n",
    "\n",
    "- Always measure performance on data not used during training.\n",
    "- Choose metrics that align with business goals (e.g., recall for medical tests).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56caa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance evaluation examples\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Dummy classification example\n",
    "y_true = np.array([0, 1, 1, 0, 1, 0])  # true labels\n",
    "y_pred = np.array([0, 0, 1, 0, 1, 1])  # predicted labels\n",
    "print('Accuracy:', accuracy_score(y_true, y_pred))  # accuracy\n",
    "print('Precision (macro):', precision_score(y_true, y_pred, average='macro'))  # precision\n",
    "print('Recall (macro):', recall_score(y_true, y_pred, average='macro'))  # recall\n",
    "print('F1 (macro):', f1_score(y_true, y_pred, average='macro'))  # f1\n",
    "\n",
    "# Dummy regression example\n",
    "y_true_reg = np.array([2.5, 0.0, 2.1, 7.8])  # true continuous targets\n",
    "y_pred_reg = np.array([3.0, -0.1, 2.0, 7.5])  # predicted targets\n",
    "print('MSE:', mean_squared_error(y_true_reg, y_pred_reg))  # mean squared error\n",
    "print('R2:', r2_score(y_true_reg, y_pred_reg))  # R-squared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82665e1",
   "metadata": {},
   "source": [
    "## Naive Bayes Optimizations\n",
    "\n",
    "### Variants & Optimizations\n",
    "\n",
    "Different Naive Bayes variants suit different data types and tasks:\n",
    "\n",
    "- **GaussianNB**: models continuous features with Gaussian likelihoods.\n",
    "- **MultinomialNB**: models discrete count features (e.g., word counts); useful for NLP.\n",
    "- **BernoulliNB**: models binary features (presence/absence).\n",
    "\n",
    "**Laplace (additive) smoothing** prevents zero-likelihood by adding \\(\\alpha>0\\) to counts:\n",
    "\n",
    "\\[ P(w \\mid C) = \\frac{count(w, C) + \\alpha}{\\sum_{w'} count(w', C) + \\alpha |V|} \\]\n",
    "\n",
    "**Feature selection** (e.g., chi-square, mutual information) often improves Naive Bayes by removing noisy features. **Calibration** and combining NB in ensembles (e.g., via stacking) can improve probability estimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dfe690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes optimizations demonstration: using TF-IDF, feature selection, and MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "texts = [\n",
    "    'buy cheap meds now',\n",
    "    'limited time offer buy',\n",
    "    'project meeting schedule',\n",
    "    'let us discuss the project plan'\n",
    "]\n",
    "labels = [1, 1, 0, 0]\n",
    "\n",
    "# Build a pipeline: TF-IDF -> select top-k features by chi2 -> MultinomialNB\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),  # convert raw text to TF-IDF features\n",
    "    SelectKBest(chi2, k=5),  # pick top 5 features by chi-squared test\n",
    "    MultinomialNB(alpha=1.0)  # classifier with Laplace smoothing\n",
    ")\n",
    "pipeline.fit(texts, labels)  # train pipeline\n",
    "print('Prediction (optimized pipeline):', pipeline.predict(['cheap limited offer']))  # test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac15944",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)\n",
    "\n",
    "### Definition & Working Principle\n",
    "\n",
    "K-Nearest Neighbors (KNN) is an instance-based (lazy) algorithm that predicts the label of a query point by looking at the labels of the k nearest training examples according to a distance metric.\n",
    "\n",
    "### Distance metrics\n",
    "\n",
    "- **Euclidean distance:** \\(d(x,y)=\\sqrt{\\sum_i (x_i-y_i)^2}\\)\n",
    "- **Manhattan distance:** \\(d(x,y)=\\sum_i |x_i-y_i|\\)\n",
    "- **Cosine similarity** (for angles) often used for text embeddings.\n",
    "\n",
    "### Key choices\n",
    "\n",
    "- **k (neighbors)**: small k â†’ low bias/high variance; large k â†’ high bias/low variance.\n",
    "- **Distance weighting**: neighbors can be weighted inversely by distance.\n",
    "\n",
    "### Strengths & Limitations\n",
    "\n",
    "- Strengths: simple, no training cost, can model complex decision boundaries.\n",
    "- Limitations: expensive at prediction time (needs full dataset), sensitive to feature scaling and irrelevant features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN example with comments\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Using a small toy dataset\n",
    "X = [[1.0, 2.0], [1.1, 1.9], [4.0, 4.1], [4.2, 3.9]]  # toy 2D points\n",
    "y = [0, 0, 1, 1]  # labels for two clusters\n",
    "\n",
    "# Build a pipeline to standardize features then apply KNN\n",
    "knn_pipeline = make_pipeline(\n",
    "    StandardScaler(),  # scale features to zero mean and unit variance\n",
    "    KNeighborsClassifier(n_neighbors=3)  # KNN with k=3\n",
    ")\n",
    "knn_pipeline.fit(X, y)  # store training points in the pipeline\n",
    "print('KNN prediction for [2.0,2.0]:', knn_pipeline.predict([[2.0, 2.0]]))  # query point\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f972813",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "### Definition & Working Principle\n",
    "\n",
    "Decision Trees split data recursively along features to form a tree where leaves represent predictions. At each split, the algorithm chooses the feature and threshold that best improves a purity metric.\n",
    "\n",
    "### Purity measures\n",
    "\n",
    "- **Entropy:** \\(H(S) = -\\sum_{i} p_i \\log_2 p_i\\)\n",
    "- **Information Gain:** difference in entropy before and after the split.\n",
    "- **Gini impurity:** \\(G = 1 - \\sum_i p_i^2\\)\n",
    "\n",
    "### Tree building (high-level)\n",
    "\n",
    "1. Start with all training data at the root.\n",
    "2. For each candidate split (feature + threshold), compute impurity reduction.\n",
    "3. Choose split with maximum reduction and recurse on children.\n",
    "4. Stop when stopping criteria met (max depth, min samples, pure node).\n",
    "\n",
    "### Overfitting & Regularization\n",
    "\n",
    "- Trees can overfit by creating deep structures that memorize training data.\n",
    "- Remedies: pruning, limiting max depth, min samples per leaf, ensemble methods (Random Forests, Gradient Boosting).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042113a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree example with comments\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# toy dataset\n",
    "X = [[0, 0], [1, 1], [0, 1], [1, 0]]  # simple binary feature pairs\n",
    "y = [0, 1, 1, 0]\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=42)  # entropy criterion\n",
    "dt.fit(X, y)  # train the decision tree\n",
    "print('Decision Tree predictions:', dt.predict(X))  # predictable on training set\n",
    "\n",
    "# Visualize tree (if running interactively this will plot)\n",
    "plt.figure(figsize=(6,4))  # set figure size\n",
    "plot_tree(dt, feature_names=['f1','f2'], class_names=['class0','class1'], filled=True)  # draw tree\n",
    "plt.title('Decision Tree (toy example)')  # title\n",
    "plt.show()  # display plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf58ddd",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "### Definition & Working Principle\n",
    "\n",
    "Linear Regression models the expected value of a continuous target as a linear combination of input features:\n",
    "\n",
    "\\[ y = \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_j + \\epsilon \\]\n",
    "\n",
    "Parameters \\(\\beta\\) are estimated by minimizing the sum of squared residuals (Ordinary Least Squares):\n",
    "\n",
    "\\[ \\hat{\\beta} = \\arg\\min_{\\beta} \\sum_{i=1}^n (y_i - X_i \\beta)^2 \\]\n",
    "\n",
    "Closed-form solution (normal equation) when \\(X^TX\\) is invertible:\n",
    "\n",
    "\\[ \\hat{\\beta} = (X^T X)^{-1} X^T y \\]\n",
    "\n",
    "### Regularization\n",
    "\n",
    "- **Ridge (L2)** adds \\(\\lambda ||\\beta||_2^2\\) penalty to reduce variance.\n",
    "- **Lasso (L1)** adds \\(\\lambda ||\\beta||_1\\) penalty and can perform feature selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0065d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression example with comments\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "# simple linear data: y = 2x + noise\n",
    "X = np.arange(1, 6).reshape(-1, 1).astype(float)  # feature matrix (5 samples)\n",
    "y = (2 * X).ravel() + np.random.normal(scale=0.1, size=X.shape[0])  # target with small noise\n",
    "\n",
    "lr = LinearRegression()  # ordinary least squares\n",
    "lr.fit(X, y)  # fit model\n",
    "print('Linear Regression coef (slope):', lr.coef_, 'intercept:', lr.intercept_)  # parameters\n",
    "print('Predict for x=6:', lr.predict([[6]]))  # predict\n",
    "\n",
    "# Ridge and Lasso example\n",
    "ridge = Ridge(alpha=1.0)  # L2 regularization with strength 1.0\n",
    "lasso = Lasso(alpha=0.1)  # L1 regularization\n",
    "ridge.fit(X, y)  # fit ridge\n",
    "lasso.fit(X, y)  # fit lasso\n",
    "print('Ridge coef:', ridge.coef_)  # ridge coef\n",
    "print('Lasso coef:', lasso.coef_)  # lasso coef (may be shrunk to zero)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d161a43",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Definition & Working Principle\n",
    "\n",
    "Logistic Regression is a linear model for classification that models the log-odds (logit) of the probability of the positive class as a linear function of features. The probability is obtained via the sigmoid (logistic) function:\n",
    "\n",
    "\\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\quad\\text{with } z = \\beta_0 + \\sum_j \\beta_j x_j \\]\n",
    "\n",
    "Log-loss (negative log-likelihood) is minimized to estimate parameters:\n",
    "\n",
    "\\[ J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^m \\left[y^{(i)} \\log(h_\\beta(x^{(i)})) + (1-y^{(i)})\\log(1-h_\\beta(x^{(i)}))\\right] \\]\n",
    "\n",
    "Multiclass logistic regression can be implemented via one-vs-rest (OvR) or by using a multinomial (softmax) formulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b5e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression example with comments\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# synthetic binary classification dataset\n",
    "X, y = make_classification(n_samples=200, n_features=4, n_informative=2, n_redundant=0, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression(max_iter=200)  # logistic regression solver default\n",
    "logreg.fit(X_train, y_train)  # train model\n",
    "y_pred = logreg.predict(X_test)  # predict\n",
    "print('Logistic Regression classification report:')  # label\n",
    "print(classification_report(y_test, y_pred))  # show metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d5b0b3",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)\n",
    "\n",
    "### Definition & Working Principle\n",
    "\n",
    "Support Vector Machine (SVM) finds a hyperplane that maximizes the margin between classes. For linearly separable data the hard-margin formulation searches for \\(w, b\\) that minimize \\(\\frac{1}{2}||w||^2\\) subject to constraints \\(y_i(w\\cdot x_i + b) \\ge 1\\).\n",
    "\n",
    "For non-separable data the soft-margin adds slack variables and a penalty parameter \\(C\\) to control misclassification.\n",
    "\n",
    "### Kernel trick\n",
    "\n",
    "When data is not linearly separable in input space, kernels implicitly project data into higher-dimensional spaces where a linear separator may exist. Common kernels: linear, polynomial, RBF (Gaussian):\n",
    "\n",
    "RBF kernel: \\(K(x,x') = \\exp(-\\gamma ||x - x'||^2)\\)\n",
    "\n",
    "### Strengths & Limitations\n",
    "\n",
    "- Strengths: effective in high-dimensional spaces, robust with clear margins, uses support vectors (sparse solution).\n",
    "- Limitations: training can be slow on very large datasets, requires careful kernel/parameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb196809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM example with comments\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# synthetic dataset with two blobs\n",
    "X, y = make_blobs(n_samples=200, centers=2, cluster_std=1.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "svm_linear = SVC(kernel='linear', C=1.0)  # linear SVM with regularization C\n",
    "svm_linear.fit(X_train, y_train)  # train\n",
    "y_pred = svm_linear.predict(X_test)  # predict\n",
    "print('Linear SVM classification report:')  # label\n",
    "print(classification_report(y_test, y_pred))  # metrics\n",
    "\n",
    "# RBF kernel example\n",
    "svm_rbf = SVC(kernel='rbf', gamma='scale', C=1.0)  # RBF kernel\n",
    "svm_rbf.fit(X_train, y_train)  # train\n",
    "print('RBF SVM score on test:', svm_rbf.score(X_test, y_test))  # test score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09834b75",
   "metadata": {},
   "source": [
    "## Conclusion and References\n",
    "\n",
    "This notebook provided a single-source-of-truth overview of popular supervised learning algorithms including:\n",
    "\n",
    "- Theoretical definitions with mathematical formulas rendered in LaTeX.\n",
    "- Intuition and plain-English explanations.\n",
    "- Key terms, strengths, limitations, and optimization notes.\n",
    "- Runnable Python examples (scikit-learn) with line-by-line comments.\n",
    "\n",
    "**Recommended references for deeper study:**\n",
    "\n",
    "- \"Pattern Recognition and Machine Learning\" by C. Bishop\n",
    "- \"The Elements of Statistical Learning\" by Hastie, Tibshirani, and Friedman\n",
    "- scikit-learn documentation: https://scikit-learn.org\n",
    "\n",
    "---\n",
    "\n",
    "_End of notebook._"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
