{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68868dfe",
   "metadata": {},
   "source": [
    "# Supervised Learning â€” Single Source of Truth (Expanded Edition)\n",
    "\n",
    "**This notebook contains:**\n",
    "\n",
    "- Deep theoretical explanations with LaTeX-rendered formulas.\n",
    "- Mathematical derivations and proofs for core algorithms.\n",
    "- Visualizations (Matplotlib) for key formulas: Gaussian PDFs, Entropy, Sigmoid, SVM margin, Linear fit.\n",
    "- Practical, runnable scikit-learn examples using real and synthetic datasets.\n",
    "- Step-by-step commented code and a final comparison summary.\n",
    "\n",
    "**Algorithms covered:** Naive Bayes, Performance Evaluation, Naive Bayes Optimizations, KNN, Decision Trees, Linear Regression, Logistic Regression, Support Vector Machines.\n",
    "\n",
    "---\n",
    "\n",
    "_Generated programmatically. Run cells in order to reproduce figures and analyses._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce82ac",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Naive Bayes](#Naive-Bayes)\n",
    "2. [Performance Evaluation](#Performance-Evaluation)\n",
    "3. [Naive Bayes Optimizations](#Naive-Bayes-Optimizations)\n",
    "4. [K-Nearest Neighbors (KNN)](#K-Nearest-Neighbors)\n",
    "5. [Decision Trees](#Decision-Trees)\n",
    "6. [Linear Regression](#Linear-Regression)\n",
    "7. [Logistic Regression](#Logistic-Regression)\n",
    "8. [Support Vector Machine (SVM)](#Support-Vector-Machine)\n",
    "9. [Summary Comparison Table](#Summary-Comparison)\n",
    "10. [References](#References)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ccce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports used across examples\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log, sqrt, exp\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure plots show inline when run in a Jupyter environment\n",
    "%matplotlib inline\n",
    "\n",
    "print('Libraries imported. Run individual sections to reproduce figures and experiments.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb2fe61",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "### Definition & Intuition\n",
    "\n",
    "Naive Bayes is a family of simple probabilistic classifiers based on **Bayes' theorem** with the strong (naive) assumption that features are conditionally independent given the class label. Despite this simplification, Naive Bayes often performs well in practice, especially on high-dimensional tasks such as text classification.\n",
    "\n",
    "### Bayes' Theorem\n",
    "\n",
    "$$\n",
    "P(C_k \\mid X) = \\frac{P(X \\mid C_k) P(C_k)}{P(X)}\n",
    "$$\n",
    "\n",
    "where \\(X=(x_1,\\dots,x_n)\\) are features and \\(C_k\\) is the k-th class.\n",
    "\n",
    "Under the naive independence assumption:\n",
    "\n",
    "$$\n",
    "P(X \\mid C_k) = \\prod_{i=1}^n P(x_i \\mid C_k)\n",
    "$$\n",
    "\n",
    "so posterior becomes (up to normalization):\n",
    "\n",
    "$$\n",
    "P(C_k \\mid X) \\propto P(C_k) \\prod_{i=1}^n P(x_i \\mid C_k)\n",
    "$$\n",
    "\n",
    "### Variants and Likelihood Forms\n",
    "\n",
    "- **Gaussian Naive Bayes (continuous features):**\n",
    "\n",
    "$$\n",
    "P(x_i \\mid C_k) = \\frac{1}{\\sqrt{2\\pi \\sigma_{k,i}^2}} \\exp\\left(-\\frac{(x_i-\\mu_{k,i})^2}{2\\sigma_{k,i}^2}\\right)\n",
    "$$\n",
    "\n",
    "- **Multinomial Naive Bayes (counts):** likelihood proportional to counts of tokens given class.\n",
    "\n",
    "- **Bernoulli Naive Bayes (binary features):** models presence/absence of features.\n",
    "\n",
    "### Working Steps (Gaussian NB example)\n",
    "\n",
    "1. Estimate class priors \\(\\hat{P}(C_k) = \\frac{N_k}{N}\\).\n",
    "2. For each class and each feature, estimate mean \\(\\mu_{k,i}\\) and variance \\(\\sigma_{k,i}^2\\) from training data.\n",
    "3. For a test sample compute class-conditional likelihoods using Gaussian density and multiply by priors.\n",
    "4. Predict class with highest posterior.\n",
    "\n",
    "### Mathematical note on log-probabilities\n",
    "\n",
    "Because product of many probabilities can underflow, we usually compute log-probabilities:\n",
    "\n",
    "$$\n",
    "\\log P(C_k \\mid X) = \\log P(C_k) + \\sum_{i=1}^n \\log P(x_i \\mid C_k) + \\text{constant}\n",
    "$$\n",
    "\n",
    "### Proof sketch (why multiply likelihoods?)\n",
    "\n",
    "From conditional independence: \\(P(x_1,x_2|C)=P(x_1|C)P(x_2|C)\\). Repeated application yields product across features. This is simply the chain rule simplified by independence assumptions.\n",
    "\n",
    "### Visualization objectives\n",
    "\n",
    "- Plot Gaussian class-conditional densities for two classes along a single feature axis to show separation.\n",
    "- Show how class posterior changes with x.\n",
    "\n",
    "### Example & Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517a5bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes visualization and Iris example\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1D Gaussian PDF visualization for two classes\n",
    "def gaussian_pdf(x, mu, sigma):\n",
    "    return (1.0 / (np.sqrt(2 * np.pi) * sigma)) * np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "x = np.linspace(-5, 10, 400)\n",
    "mu0, sigma0 = 0.0, 1.0\n",
    "mu1, sigma1 = 3.0, 1.2\n",
    "y0 = gaussian_pdf(x, mu0, sigma0)\n",
    "y1 = gaussian_pdf(x, mu1, sigma1)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(x, y0)\n",
    "plt.plot(x, y1)\n",
    "plt.title('Gaussian class-conditional densities (example)')\n",
    "plt.xlabel('feature value (x)')\n",
    "plt.ylabel('density')\n",
    "plt.legend(['class 0', 'class 1'])\n",
    "plt.show()\n",
    "\n",
    "# Posterior example with equal priors\n",
    "prior0 = 0.5\n",
    "prior1 = 0.5\n",
    "post0 = prior0 * y0\n",
    "post1 = prior1 * y1\n",
    "norm = post0 + post1\n",
    "posterior0 = post0 / norm\n",
    "posterior1 = post1 / norm\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(x, posterior0)\n",
    "plt.plot(x, posterior1)\n",
    "plt.title('Posterior probabilities for two classes (equal priors)')\n",
    "plt.xlabel('feature value (x)')\n",
    "plt.ylabel('P(class | x)')\n",
    "plt.legend(['P(class0|x)', 'P(class1|x)'])\n",
    "plt.show()\n",
    "\n",
    "# Practical example: GaussianNB on Iris (first two features for visualization)\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # take two features for easy plotting\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "print('GaussianNB on Iris (2 features):')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Decision boundary visualization helper\n",
    "def plot_decision_boundary(clf, X, y, title='Decision boundary'):\n",
    "    # create a mesh to plot in\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "    plt.scatter(X[:,0], X[:,1], c=y)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(gnb, X_test, y_test, title='GaussianNB Decision Boundary (Iris, 2 features)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2e5eed",
   "metadata": {},
   "source": [
    "## Performance Evaluation (Expanded)\n",
    "\n",
    "### Why it matters\n",
    "\n",
    "Performance metrics quantify how well a model will perform on unseen data and guide choices such as which model to deploy, which hyperparameters to prefer, and how to handle class imbalance.\n",
    "\n",
    "### Confusion matrix and derived metrics\n",
    "\n",
    "Confusion matrix for binary classification:\n",
    "\n",
    "\\[\n",
    "\\begin{pmatrix}\n",
    "TP & FP \\\\\n",
    "FN & TN\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "Formulas:\n",
    "\n",
    "$$Accuracy=\\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "$$Precision=\\frac{TP}{TP+FP},\\quad Recall=\\frac{TP}{TP+FN}$$\n",
    "\n",
    "$$F1=2\\cdot\\frac{Precision\\cdot Recall}{Precision+Recall}$$\n",
    "\n",
    "### ROC and AUC\n",
    "\n",
    "- Receiver Operating Characteristic (ROC) curve plots True Positive Rate vs False Positive Rate as the threshold varies.\n",
    "- AUC is the area under ROC, a threshold-independent measure of separability.\n",
    "\n",
    "### Cross-validation\n",
    "\n",
    "- k-fold cross-validation: split data into k parts, train on k-1 parts, validate on the remaining part, repeat k times.\n",
    "- Stratified k-fold preserves class ratios.\n",
    "\n",
    "### Visualization objectives\n",
    "\n",
    "- Plot ROC curve for a classifier on a binary problem.\n",
    "- Show how precision/recall change with threshold (Precision-Recall curve).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddc3511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance evaluation visualizations\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "\n",
    "# synthetic binary dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "clf = LogisticRegression(max_iter=300)\n",
    "clf.fit(X_train, y_train)\n",
    "y_score = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.title(f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision)\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a30cf5a",
   "metadata": {},
   "source": [
    "## Naive Bayes Optimizations (Expanded)\n",
    "\n",
    "### Smoothing\n",
    "\n",
    "Laplace (additive) smoothing avoids zero probabilities by adding \\(\\alpha\\) to counts:\n",
    "\n",
    "$$P(w\\mid C)=\\frac{count(w,C)+\\alpha}{\\sum_{w'}count(w',C)+\\alpha |V|}$$\n",
    "\n",
    "### Feature selection & dimensionality reduction\n",
    "\n",
    "- Techniques such as chi-squared test, mutual information, and TF-IDF weighting improve Naive Bayes on text tasks.\n",
    "\n",
    "### Calibration and Ensembles\n",
    "\n",
    "- Calibration (e.g., isotonic regression, Platt scaling) can make predicted probabilities more accurate.\n",
    "- Ensembles combining Naive Bayes with other weak learners can boost performance.\n",
    "\n",
    "### Visualization goals\n",
    "\n",
    "- Show effect of smoothing on probability estimates for rare tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e421da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Laplace smoothing effect on token probability estimates\n",
    "from collections import Counter\n",
    "\n",
    "docs_spam = ['buy cheap meds', 'limited offer buy now', 'cheap meds available']\n",
    "docs_ham = ['project meeting schedule', 'discussion about project', 'let us meet']\n",
    "vocab = set(' '.join(docs_spam + docs_ham).split())\n",
    "V = len(vocab)\n",
    "\n",
    "def estimate_prob(token, docs, alpha):\n",
    "    counts = Counter(' '.join(docs).split())\n",
    "    total = sum(counts.values())\n",
    "    return (counts[token] + alpha) / (total + alpha * V)\n",
    "\n",
    "tokens = list(vocab)\n",
    "for alpha in [0.0, 0.5, 1.0]:\n",
    "    probs = [estimate_prob(t, docs_spam, alpha) for t in tokens]\n",
    "    print(f'alpha={alpha}, sample probs (first 5):', probs[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96000a2",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN) â€” Expanded\n",
    "\n",
    "### Working principle\n",
    "\n",
    "KNN stores all training samples and, for a query point, finds the k nearest neighbors by a distance metric and predicts by majority vote (classification) or averaging (regression).\n",
    "\n",
    "### Distance metrics\n",
    "\n",
    "- Euclidean: \\(d(x,y)=\\sqrt{\\sum (x_i-y_i)^2}\\)\n",
    "- Manhattan: \\(d(x,y)=\\sum |x_i-y_i|\\)\n",
    "- Cosine similarity: measures angle between vectors, useful for high-dimensional sparse data.\n",
    "\n",
    "### Complexity\n",
    "\n",
    "- Training: O(1) (stores data)\n",
    "- Prediction: O(n * d) per query (can be reduced with KD-trees or Ball trees for low to moderate dimensions).\n",
    "\n",
    "### Visualization goals\n",
    "\n",
    "- Show how decision boundary changes as k varies on a 2D toy dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9bf4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN visualization: decision boundary for different k values\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "def plot_knn_k(k):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    # mesh\n",
    "    x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n",
    "    y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min,x_max,200), np.linspace(y_min,y_max,200))\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "    plt.scatter(X[:,0], X[:,1], c=y)\n",
    "    plt.title(f'KNN decision boundary (k={k})')\n",
    "    plt.show()\n",
    "\n",
    "for k in [1,3,9]:\n",
    "    plot_knn_k(k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f901cc71",
   "metadata": {},
   "source": [
    "## Decision Trees â€” Expanded\n",
    "\n",
    "### Split criteria\n",
    "\n",
    "- **Entropy:**\n",
    "\n",
    "$$Entropy(S) = -\\sum_{i=1}^c p_i \\log_2 p_i$$\n",
    "\n",
    "- **Information Gain:** reduction in entropy after a split.\n",
    "\n",
    "- **Gini impurity:**\n",
    "\n",
    "$$Gini(S) = 1 - \\sum_{i=1}^c p_i^2$$\n",
    "\n",
    "### Proof sketch: why choose the split with highest information gain?\n",
    "\n",
    "Splitting that reduces uncertainty (entropy) the most yields children that are purer and thus easier to classify. Information gain is simply parent entropy minus weighted child entropy.\n",
    "\n",
    "### Visualization goals\n",
    "\n",
    "- Plot entropy vs class probability p to show where entropy is highest (p=0.5).\n",
    "- Train a small tree and visualize splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d0bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree entropy plot and small example\n",
    "ps = np.linspace(0,1,500)\n",
    "entropy = - (ps * np.log2(ps + 1e-12) + (1-ps) * np.log2(1-ps + 1e-12))\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(ps, entropy)\n",
    "plt.title('Binary entropy as a function of p')\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('Entropy')\n",
    "plt.show()\n",
    "\n",
    "# Small decision tree on synthetic data\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.25, random_state=42)\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dt.fit(X, y)\n",
    "plt.figure(figsize=(6,4))\n",
    "plot_tree(dt, filled=True)\n",
    "plt.title('Decision Tree (make_moons)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316bc44e",
   "metadata": {},
   "source": [
    "## Linear Regression â€” Expanded\n",
    "\n",
    "### Model and OLS derivation (normal equation)\n",
    "\n",
    "Model: $$y = X\\beta + \\epsilon$$\n",
    "Ordinary Least Squares minimizes RSS:\n",
    "\n",
    "$$J(\\beta) = (y - X\\beta)^T(y - X\\beta)$$\n",
    "Set derivative w.r.t. \\(\\beta\\) to zero to get normal equation:\n",
    "\n",
    "$$-2X^T(y - X\\beta) = 0 \\Rightarrow X^TX\\beta = X^T y$$\n",
    "Provided \\(X^TX\\) invertible:\n",
    "\n",
    "$$\\hat{\\beta} = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "### Visualization goals\n",
    "\n",
    "- Fit a simple linear regression to noisy data and plot the best-fit line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression fit and visualization\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 50)\n",
    "y = 2.5 * X + 1.0 + np.random.normal(scale=3.0, size=X.shape)\n",
    "X_mat = X.reshape(-1,1)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_mat, y)\n",
    "y_pred = lr.predict(X_mat)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, y_pred)\n",
    "plt.title('Linear Regression fit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "print('Estimated coefficient:', lr.coef_, 'intercept:', lr.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2597826c",
   "metadata": {},
   "source": [
    "## Logistic Regression â€” Expanded\n",
    "\n",
    "### Model and likelihood\n",
    "\n",
    "Logistic regression models probability via the sigmoid function:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Given dataset, the likelihood of parameters is:\n",
    "\n",
    "$$L(\\beta) = \\prod_{i=1}^m \\sigma(x^{(i)}\\beta)^{y^{(i)}} (1-\\sigma(x^{(i)}\\beta))^{1-y^{(i)}}$$\n",
    "Negative log-likelihood (loss):\n",
    "\n",
    "$$J(\\beta) = -\\sum_{i=1}^m \\left[y^{(i)}\\log \\sigma(x^{(i)}\\beta) + (1-y^{(i)})\\log(1-\\sigma(x^{(i)}\\beta))\\right]$$\n",
    "\n",
    "### Visualization goals\n",
    "\n",
    "- Plot the sigmoid function and show how linear input maps to probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d3ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid visualization and logistic regression example\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "z = np.linspace(-10, 10, 400)\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(z, sigmoid(z))\n",
    "plt.title('Sigmoid function')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('sigma(z)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Logistic regression on synthetic separable data\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0, n_informative=2, random_state=42)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)\n",
    "print('Logistic Regression score:', clf.score(X, y))\n",
    "\n",
    "# Plot decision boundary\n",
    "xx, yy = np.meshgrid(np.linspace(X[:,0].min()-1, X[:,0].max()+1, 200), np.linspace(X[:,1].min()-1, X[:,1].max()+1, 200))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "plt.title('Logistic Regression decision boundary')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299b78db",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM) â€” Expanded\n",
    "\n",
    "### Hard-margin objective (linearly separable)\n",
    "\n",
    "Minimize:\n",
    "\n",
    "$$\\frac{1}{2} ||w||^2$$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "$$y_i(w\\cdot x_i + b) \\ge 1, \\quad \\forall i$$\n",
    "\n",
    "Margin between classes equals \\(2/||w||\\).\n",
    "\n",
    "### Soft-margin (non-separable)\n",
    "\n",
    "Introduce slack variables \\(\\xi_i\\) and penalty parameter \\(C\\):\n",
    "\n",
    "$$\\min \\frac{1}{2}||w||^2 + C \\sum_i \\xi_i$$\n",
    "subject to \\(y_i(w\\cdot x_i + b) \\ge 1 - \\xi_i, \\xi_i \\ge 0\\).\n",
    "\n",
    "### Kernel trick\n",
    "\n",
    "Replace dot product with kernel function \\(K(x,x')\\) to operate in implicit feature space.\n",
    "\n",
    "### Visualization goals\n",
    "\n",
    "- Plot SVM decision boundary, support vectors, and margin lines on a 2D dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f0966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM margin visualization\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=100, centers=2, random_state=42)\n",
    "clf = SVC(kernel='linear', C=1.0)\n",
    "clf.fit(X, y)\n",
    "plt.figure(figsize=(6,4))\n",
    "xx = np.linspace(X[:,0].min()-1, X[:,0].max()+1, 200)\n",
    "yy = np.linspace(X[:,1].min()-1, X[:,1].max()+1, 200)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Z = clf.predict(np.c_[XX.ravel(), YY.ravel()])\n",
    "Z = Z.reshape(XX.shape)\n",
    "plt.contourf(XX, YY, Z, alpha=0.3)\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "# plot support vectors\n",
    "sv = clf.support_vectors_\n",
    "plt.scatter(sv[:,0], sv[:,1], s=100, facecolors='none', edgecolors='k')\n",
    "plt.title('Linear SVM with support vectors (circled)')\n",
    "plt.show()\n",
    "\n",
    "print('Number of support vectors for each class:', clf.n_support_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5987a598",
   "metadata": {},
   "source": [
    "## Summary Comparison\n",
    "\n",
    "| Algorithm | Type | Key Idea | Strength | Limitation | Common Use Case |\n",
    "|---|---|---|---|---|---|\n",
    "| Naive Bayes | Probabilistic | Bayes rule with feature independence | Fast, works well on text | Independence assumption | Spam detection |\n",
    "| KNN | Instance-based | Majority vote among neighbors | Simple, non-parametric | Slow at prediction | Small-scale classification |\n",
    "| Decision Tree | Tree-based | Recursive partitioning | Interpretable | Overfits easily | Classification with mixed features |\n",
    "| Linear Regression | Parametric (regression) | Fit linear model via OLS | Interpretable | Poor for non-linear data | Trend estimation |\n",
    "| Logistic Regression | Parametric (classification) | Linear model on log-odds via sigmoid | Probabilistic outputs | Linear decision boundary | Binary classification |\n",
    "| SVM | Margin-based | Maximize margin using support vectors | Effective in high-dimensions | Slow on large datasets | Text classification, bioinformatics |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cafe2ec",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Bishop, C. M. *Pattern Recognition and Machine Learning*.\n",
    "- Hastie, Tibshirani, Friedman. *The Elements of Statistical Learning*.\n",
    "- scikit-learn documentation: https://scikit-learn.org\n",
    "- Goodfellow, Bengio, Courville. *Deep Learning* (for broader context).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
