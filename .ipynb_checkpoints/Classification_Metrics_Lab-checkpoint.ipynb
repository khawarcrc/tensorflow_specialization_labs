{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f84fca57",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Part 2: Classification Metrics\n",
    "\n",
    "In this section, we will cover the most important **classification metrics** used in Machine Learning and Deep Learning.\n",
    "\n",
    "These metrics are essential for evaluating models on **binary classification** and **multi-class classification** problems.\n",
    "\n",
    "We will cover:\n",
    "1. Accuracy\n",
    "2. Precision\n",
    "3. Recall\n",
    "4. F1 Score\n",
    "5. ROC & AUC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802172a5",
   "metadata": {},
   "source": [
    "## 1. Accuracy\n",
    "\n",
    "**Definition:**\n",
    "Accuracy measures the proportion of correctly classified instances (both positive and negative) out of the total instances.\n",
    "\n",
    "**Why it is used in ML:**\n",
    "It provides a simple measure of performance when classes are balanced.\n",
    "\n",
    "**Pros:** Easy to interpret, widely used.\n",
    "**Cons:** Misleading for imbalanced datasets (e.g., 95% negatives).\n",
    "\n",
    "$$ Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eb8240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Accuracy Calculation\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0])\n",
    "y_pred = np.array([1, 0, 1, 0, 0, 1, 0, 1])\n",
    "\n",
    "# Manual\n",
    "accuracy_manual = np.sum(y_true == y_pred) / len(y_true)\n",
    "print('Manual Accuracy:', accuracy_manual)\n",
    "\n",
    "# Sklearn\n",
    "print('Sklearn Accuracy:', accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccccd26c",
   "metadata": {},
   "source": [
    "## 2. Precision\n",
    "\n",
    "**Definition:**\n",
    "Precision measures the proportion of correctly predicted positive observations out of all predicted positives.\n",
    "\n",
    "**Why it is used in ML:**\n",
    "Useful when the cost of **false positives** is high.\n",
    "\n",
    "**Pros:** Important in information retrieval (search engines, spam filters).\n",
    "**Cons:** Ignores false negatives.\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP + FP} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d174e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision_manual = np.sum((y_true == 1) & (y_pred == 1)) / np.sum(y_pred == 1)\n",
    "print('Manual Precision:', precision_manual)\n",
    "\n",
    "print('Sklearn Precision:', precision_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc95fb1",
   "metadata": {},
   "source": [
    "## 3. Recall (Sensitivity)\n",
    "\n",
    "**Definition:**\n",
    "Recall measures the proportion of correctly predicted positive observations out of all actual positives.\n",
    "\n",
    "**Why it is used in ML:**\n",
    "Important when missing a positive case is very costly (e.g., medical diagnosis).\n",
    "\n",
    "**Pros:** Prioritizes catching all positive cases.\n",
    "**Cons:** Can be high while precision is low.\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP + FN} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e54db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall_manual = np.sum((y_true == 1) & (y_pred == 1)) / np.sum(y_true == 1)\n",
    "print('Manual Recall:', recall_manual)\n",
    "\n",
    "print('Sklearn Recall:', recall_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a5915a",
   "metadata": {},
   "source": [
    "## 4. F1 Score\n",
    "\n",
    "**Definition:**\n",
    "The F1 score is the harmonic mean of Precision and Recall.\n",
    "\n",
    "**Why it is used in ML:**\n",
    "Balances the trade-off between Precision and Recall.\n",
    "\n",
    "**Pros:** Good when you need a balance between precision and recall.\n",
    "**Cons:** Not intuitive to interpret compared to accuracy.\n",
    "\n",
    "$$ F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa18510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1_manual = 2 * (precision * recall) / (precision + recall)\n",
    "print('Manual F1 Score:', f1_manual)\n",
    "\n",
    "print('Sklearn F1 Score:', f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b028273",
   "metadata": {},
   "source": [
    "## 5. ROC Curve & AUC\n",
    "\n",
    "**Definition:**\n",
    "The ROC curve plots True Positive Rate (Recall) against False Positive Rate at different thresholds. The AUC measures the overall area under this curve.\n",
    "\n",
    "**Why it is used in ML:**\n",
    "Useful for comparing classifiers, especially with imbalanced data.\n",
    "\n",
    "**Pros:** Threshold-independent evaluation.\n",
    "**Cons:** Can be misleading with highly imbalanced datasets.\n",
    "\n",
    "$$ TPR = \\frac{TP}{TP + FN} $$\n",
    "$$ FPR = \\frac{FP}{FP + TN} $$\n",
    "\n",
    "$$ AUC = \\int_0^1 TPR(FPR) \\, dFPR $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7b3157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_scores = np.array([0.9, 0.2, 0.8, 0.4, 0.3, 0.7, 0.1, 0.6])  # Probabilities\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xz-tf-leraning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
