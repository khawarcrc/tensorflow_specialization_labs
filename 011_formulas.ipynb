{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Statistical Metrics and Concepts in Machine Learning & Forecasting: Master Lab Notebook",
        "",
        "This notebook is your **single source of truth** for key statistical metrics and concepts used in Machine Learning (ML) and forecasting. It covers error/loss metrics, classification metrics, foundational statistical concepts, and other useful metrics. Each concept follows a consistent structure for clarity and ease of reference.",
        "",
        "We'll use synthetic data for demonstrations. For regression: linear data with noise. For classification: binary labels.",
        "",
        "**Prerequisites:** Run the following cell to import necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np",
        "import tensorflow as tf",
        "from tensorflow.keras import backend as K",
        "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall, AUC, BinaryCrossentropy",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score",
        "import matplotlib.pyplot as plt",
        "from sklearn.model_selection import train_test_split",
        "from sklearn.metrics import mean_absolute_percentage_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Synthetic Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Regression data",
        "np.random.seed(42)",
        "X_reg = np.random.rand(100, 1) * 10",
        "y_reg_true = 2 * X_reg.squeeze() + 1 + np.random.randn(100) * 0.5  # True: y = 2x + 1 + noise",
        "y_reg_pred = 2 * X_reg.squeeze() + 1 + np.random.randn(100) * 0.8  # Predictions with more noise",
        "",
        "# Classification data (binary)",
        "X_class = np.random.rand(100, 1)",
        "y_class_true = (X_class.squeeze() > 0.5).astype(int)",
        "y_class_pred_prob = np.random.rand(100)  # Probabilities",
        "y_class_pred_binary = (y_class_pred_prob > 0.5).astype(int)  # Binary predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Error & Loss Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Mean Squared Error (MSE)",
        "",
        "#### Definition",
        "Mean Squared Error (MSE) quantifies the average squared difference between predicted and actual values, emphasizing larger errors due to squaring.",
        "",
        "#### Why It Is Used in ML / Forecasting",
        "In ML regression and forecasting (e.g., time series prediction), MSE serves as a loss function to minimize during training, promoting models that reduce variance in residuals. It's ideal for Gaussian-distributed errors.",
        "",
        "#### Pros & Cons",
        "**Pros**: Differentiable for optimization; penalizes outliers heavily, useful in high-precision forecasting.  ",
        "**Cons**: Sensitive to outliers; results in squared units, reducing interpretability.",
        "",
        "#### Equation",
        "\\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\]",
        "",
        "#### Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual Implementation",
        "def mse_manual(y_true, y_pred):",
        "    return np.mean((y_true - y_pred) ** 2)",
        "",
        "mse_val_manual = mse_manual(y_reg_true, y_reg_pred)",
        "print(f\"Manual MSE: {mse_val_manual}\")",
        "",
        "# Keras Implementation",
        "mse_keras = MeanSquaredError()",
        "mse_val_keras = mse_keras(y_reg_true, y_reg_pred).numpy()",
        "print(f\"Keras MSE: {mse_val_keras}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparison / Interpretation",
        "Both yield similar values (~0.74). Lower MSE indicates better fit; compare across models for selection. In forecasting, interpret as average squared deviation from actuals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Root Mean Squared Error (RMSE)",
        "",
        "#### Definition",
        "Root Mean Squared Error (RMSE) is the square root of MSE, providing error magnitude in the same units as the target variable.",
        "",
        "#### Why It Is Used in ML / Forecasting",
        "RMSE is popular in forecasting (e.g., demand prediction) for its interpretability in original units, while still penalizing large errors like MSE.",
        "",
        "#### Pros & Cons",
        "**Pros**: Same scale as data; good for comparing models.  ",
        "**Cons**: Still outlier-sensitive; not normalized for scale differences.",
        "",
        "#### Equation",
        "\\[ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} \\]",
        "",
        "#### Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual Implementation",
        "def rmse_manual(y_true, y_pred):",
        "    return np.sqrt(np.mean((y_true - y_pred) ** 2))",
        "",
        "rmse_val_manual = rmse_manual(y_reg_true, y_reg_pred)",
        "print(f\"Manual RMSE: {rmse_val_manual}\")",
        "",
        "# Keras Implementation (Custom, since no built-in RMSE loss, but metric possible)",
        "def rmse_keras(y_true, y_pred):",
        "    return K.sqrt(K.mean(K.square(y_pred - y_true)))",
        "",
        "rmse_val_keras = rmse_keras(y_reg_true, y_reg_pred).numpy()",
        "print(f\"Keras RMSE: {rmse_val_keras}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparison / Interpretation",
        "Both ~0.86. Interpretable as average error in units of y; useful for error magnitude in forecasting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Mean Absolute Error (MAE)",
        "",
        "#### Definition",
        "Mean Absolute Error (MAE) is the average of the absolute differences between predicted and actual values.",
        "",
        "#### Why It Is Used in ML / Forecasting",
        "Used in regression and forecasting when robustness to outliers is needed, as it treats all errors equally (e.g., in inventory forecasting).",
        "",
        "#### Pros & Cons",
        "**Pros**: Robust to outliers; interpretable in original units.  ",
        "**Cons**: Not differentiable at zero; less emphasis on large errors.",
        "",
        "#### Equation",
        "\\[ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| \\]",
        "",
        "#### Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual Implementation",
        "def mae_manual(y_true, y_pred):",
        "    return np.mean(np.abs(y_true - y_pred))",
        "",
        "mae_val_manual = mae_manual(y_reg_true, y_reg_pred)",
        "print(f\"Manual MAE: {mae_val_manual}\")",
        "",
        "# Keras Implementation",
        "mae_keras = MeanAbsoluteError()",
        "mae_val_keras = mae_keras(y_reg_true, y_reg_pred).numpy()",
        "print(f\"Keras MAE: {mae_val_keras}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparison / Interpretation",
        "Both ~0.66. Represents median error; compare to RMSE to gauge outlier impact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Mean Absolute Percentage Error (MAPE)",
        "",
        "#### Definition",
        "MAPE measures the average absolute percentage difference between predicted and actual values.",
        "",
        "#### Why It Is Used in ML / Forecasting",
        "Common in forecasting for scale-independent error (e.g., sales prediction), allowing comparison across datasets.",
        "",
        "#### Pros & Cons",
        "**Pros**: Scale-independent; intuitive as percentage.  ",
        "**Cons**: Undefined for zero actuals; biased towards low values.",
        "",
        "#### Equation",
        "\\[ \\text{MAPE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\times 100 \\]",
        "",
        "#### Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual Implementation (using sklearn)",
        "mape_val_manual = mean_absolute_percentage_error(y_reg_true, y_reg_pred) * 100",
        "print(f\"Manual MAPE: {mape_val_manual}\")",
        "",
        "# Keras Implementation (Custom)",
        "def mape_keras(y_true, y_pred):",
        "    return K.mean(K.abs((y_true - y_pred) / K.clip(K.abs(y_true), K.epsilon(), None))) * 100",
        "",
        "mape_val_keras = mape_keras(y_reg_true, y_reg_pred).numpy()",
        "print(f\"Keras MAPE: {mape_val_keras}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparison / Interpretation",
        "Both ~10-15%. Lower is better; useful for relative error in forecasting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.5 R² (Coefficient of Determination)",
        "",
        "#### Definition",
        "R² indicates the proportion of variance in the dependent variable explained by the model.",
        "",
        "#### Why It Is Used in ML / Forecasting",
        "Evaluates model fit in regression/forecasting; higher R² means better explanation of data variability.",
        "",
        "#### Pros & Cons",
        "**Pros**: Normalized (0-1); easy comparison.  ",
        "**Cons**: Can mislead in non-linear models; doesn't imply causation.",
        "",
        "#### Equation",
        "\\[ R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} \\]",
        "",
        "#### Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual Implementation",
        "def r2_manual(y_true, y_pred):",
        "    ss_res = np.sum((y_true - y_pred) ** 2)",
        "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)",
        "    return 1 - (ss_res / ss_tot)",
        "",
        "r2_val_manual = r2_manual(y_reg_true, y_reg_pred)",
        "print(f\"Manual R²: {r2_val_manual}\")",
        "",
        "# Keras Implementation (Custom)",
        "def r2_keras(y_true, y_pred):",
        "    ss_res = K.sum(K.square(y_true - y_pred))",
        "    ss_tot = K.sum(K.square(y_true - K.mean(y_true)))",
        "    return 1 - ss_res / (ss_tot + K.epsilon())",
        "",
        "r2_val_keras = r2_keras(y_reg_true, y_reg_pred).numpy()",
        "print(f\"Keras R²: {r2_val_keras}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparison / Interpretation",
        "Both ~0.97. 1 is perfect fit; negative means worse than mean predictor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Classification Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Accuracy",
        "",
        "#### Definition",
        "Accuracy is the ratio of correct predictions to total predictions.",
        "",
        "#### Why It Is Used in ML / Forecasting",
        "Basic metric for classification; used in balanced datasets (e.g., sentiment analysis).",
        "",
        "#### Pros & Cons",
        "**Pros**: Simple and intuitive.  ",
        "**Cons**: Misleading in imbalanced classes.",
        "",
        "#### Equation",
        "\\[ \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}} \\]",
        "",
        "#### Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual Implementation",
        "def accuracy_manual(y_true, y_pred):",
        "    return np.mean(y_true == y_pred)",
        "",
        "acc_val_manual = accuracy_manual(y_class_true, y_class_pred_binary)",
        "print(f\"Manual Accuracy: {acc_val_manual}\")",
        "",
        "# Keras Implementation",
        "acc_keras = Accuracy()",
        "acc_keras.update_state(y_class_true, y_class_pred_binary)",
        "acc_val_keras = acc_keras.result().numpy()",
        "print(f\"Keras Accuracy: {acc_val_keras}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparison / Interpretation",
        "Both ~0.5 (random). High accuracy good for balanced data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Precision",
        "",
        "#### Definition",
        "Precision is the ratio of true positives to predicted positives.",
        "",
        "#### Why It Is Used in ML / Forecasting",
        "Minimizes false positives (e.g., fraud detection).",
        "",
        "#### Pros & Cons",
        "**Pros**: Focuses on positive prediction quality.  ",
        "**Cons**: Ignores false negatives.",
        "",
        "#### Equation",
        "\\[ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} \\]",
        "",
        "#### Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual Implementation",
        "def precision_manual(y_true, y_pred):",
        "    tp = np.sum((y_true == 1) & (y_pred == 1))",
        "    fp = np.sum((y_true == 0) & (y_pred == 1))",
        "    return tp / (tp + fp) if (tp + fp) > 0 else 0",
        "",
        "prec_val_manual = precision_manual(y_class_true, y_class_pred_binary)",
        "print(f\"Manual Precision: {prec_val_manual}\")",
        "",
        "# Keras Implementation",
        "prec_keras = Precision()",
        "prec_keras.update_state(y_class_true, y_class_pred_binary)",
        "prec_val_keras = prec_keras.result().numpy()",
        "print(f\"Keras Precision: {prec_val_keras}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparison / Interpretation",
        "Values match. High precision reduces false alarms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Recall (Sensitivity)",
        "",
        "#### Definition",
        "Recall is the ratio of true positives to actual positives.",
        "",
        "#### Why It Is Used in ML / Forecasting",
        "Captures all positives (e.g., disease screening).",
        "",
        "#### Pros & Cons",
        "**Pros**: Minimizes missed positives.  ",
        "**Cons**: May increase false positives.",
        "",
        "#### Equation",
        "\\[ \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} \\]",
        "",
        "#### Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual Implementation",
        "def recall_manual(y_true, y_pred):",
        "    tp = np.sum((y_true == 1) & (y_pred == 1))",
        "    fn = np.sum((y_true == 1) & (y_pred == 0))",
        "    return tp / (tp + fn) if (tp + fn) > 0 else 0",
        "",
        "rec_val_manual = recall_manual(y_class_true, y_class_pred_binary)",
        "print(f\"Manual Recall: {rec_val_manual}\")",
        "",
        "# Keras Implementation",
        "rec_keras = Recall()",
        "rec_keras.update_state(y_class_true, y_class_pred_binary)",
        "rec_val_keras = rec_keras.result().numpy()",
        "print(f\"Keras Recall: {rec_val_keras}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparison / Interpretation",
        "Matching values. Trade-off with precision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Specificity",
        "",
        "#### Definition",
        "Specificity is the ratio of true negatives to actual negatives.",
        "",
        "#### Why It Is Used in ML / Forecasting",
        "Complements recall by focusing on negative class (e.g., spam filtering).",
        "",
        "#### Pros & Cons",
        "**Pros**: Good for imbalanced negatives.  ",
        "**Cons**: Ignores positives.",
        "",
        "#### Equation",
        "\\[ \\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}} \\]",
        "",
        "#### Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual Implementation",
        "def specificity_manual(y_true, y_pred):",
        "    tn = np.sum((y_true == 0) & (y_pred == 0))",
        "    fp = np.sum((y_true == 0) & (y_pred == 1))",
        "    return tn / (tn + fp) if (tn + fp) > 0 else 0",
        "",
        "spec_val_manual = specificity_manual(y_class_true, y_class_pred_binary)",
        "print(f\"Manual Specificity: {spec_val_manual}\")",
        "",
        "# Keras Implementation (Using SpecificityAtSensitivity, but for binary, custom)",
        "def specificity_keras(y_true, y_pred):",
        "    y_pred = K.round(y_pred)",
        "    tn = K.sum(K.cast((y_true == 0) & (y_pred == 0), 'float32'))",
        "    fp = K.sum(K.cast((y_true == 0) & (y_pred == 1), 'float32'))",
        "    return tn / (tn + fp + K.epsilon())",
        "",
        "spec_val_keras = specificity_keras(y_class_true, y_class_pred_binary).numpy()",
        "print(f\"Keras Specificity: {spec_val_keras}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparison / Interpretation",
        "Both match. High specificity means few false positives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 F1 Score",
        "",
        "#### Definition",
        "F1 Score is the harmonic mean of precision and recall.",
        "",
        "#### Why It Is Used in ML / Forecasting",
        "Balances precision and recall in imbalanced datasets.",
        "",
        "#### Pros & Cons",
        "**Pros**: Single metric for trade-off.  ",
        "**Cons**: Assumes equal weight; less intuitive.",
        "",
        "#### Equation",
        "\\[ F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\]",
        "",
        "#### Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual Implementation",
        "def f1_manual(y_true, y_pred):",
        "    prec = precision_manual(y_true, y_pred)",
        "    rec = recall_manual(y_true, y_pred)",
        "    return 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0",
        "",
        "f1_val_manual = f1_manual(y_class_true, y_class_pred_binary)",
        "print(f\"Manual F1: {f1_val_manual}\")",
        "",
        "# Keras Implementation (Custom)",
        "def f1_keras(y_true, y_pred):",
        "    y_pred = K.round(y_pred)",
        "    tp = K.sum(K.cast(y_true * y_pred, 'float'))",
        "    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'))",
        "    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'))",
        "    prec = tp / (tp + fp + K.epsilon())",
        "    rec = tp / (tp + fn + K.epsilon())",
        "    return 2 * (prec * rec) / (prec + rec + K.epsilon())",
        "",
        "f1_val_keras = f1_keras(y_class_true, y_class_pred_binary).numpy()",
        "print(f\"Keras F1: {f1_val_keras}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparison / Interpretation",
        "Matching. Ideal for uneven classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.6 ROC Curve & AUC",
        "",
        "#### Definition",
        "ROC plots TPR vs FPR; AUC is area under curve, measuring discriminability.",
        "",
        "#### Why It Is Used in ML / Forecasting",
        "Threshold-independent evaluation for binary classification.",
        "",
        "#### Pros & Cons",
        "**Pros**: Handles imbalance; probabilistic.  ",
        "**Cons**: Computation heavy for large data.",
        "",
        "#### Equation",
        "AUC computed via integral or trapezoidal rule.",
        "",
        "#### Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual Implementation (sklearn)",
        "auc_manual = roc_auc_score(y_class_true, y_class_pred_prob)",
        "fpr, tpr, _ = roc_curve(y_class_true, y_class_pred_prob)",
        "print(f\"Manual AUC: {auc_manual}\")",
        "",
        "# Plot ROC",
        "plt.plot(fpr, tpr)",
        "plt.show()",
        "",
        "# Keras Implementation",
        "auc_keras = AUC()",
        "auc_keras.update_state(y_class_true, y_class_pred_prob)",
        "auc_val_keras = auc_keras.result().numpy()",
        "print(f\"Keras AUC: {auc_val_keras}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparison / Interpretation",
        "Both ~0.5 (random). 1 is perfect separation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Statistical Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Bias vs Variance",
        "",
        "#### Definition",
        "Bias is error from simplistic assumptions; Variance is error from sensitivity to training data.",
        "",
        "#### Why It Is Used in ML / Forecasting",
        "Underpins trade-off in model complexity; guides regularization.",
        "",
        "#### Pros & Cons",
        "**Pros**: Conceptual framework for optimization.  ",
        "**Cons**: Hard to measure separately.",
        "",
        "#### Equation",
        "\\[ \\text{Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error} \\]",
        "",
        "#### Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# No direct code, but demonstration via overfitting/underfitting below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparison / Interpretation",
        "Balance for generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Overfitting vs Underfitting (with visualization)",
        "",
        "#### Definition",
        "Overfitting: Model too complex, fits noise. Underfitting: Too simple, misses patterns.",
        "",
        "#### Why It Is Used in ML / Forecasting",
        "Identifies poor generalization; use cross-validation to detect.",
        "",
        "#### Pros & Cons",
        "**Pros**: Guides model selection.  ",
        "**Cons**: Subjective without metrics.",
        "",
        "#### Equation",
        "No specific equation; monitored via train/test error gap.",
        "",
        "#### Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple visualization",
        "from sklearn.linear_model import LinearRegression",
        "from sklearn.preprocessing import PolynomialFeatures",
        "from sklearn.pipeline import make_pipeline",
        "",
        "X_train, X_test, y_train, y_test = train_test_split(X_reg, y_reg_true, test_size=0.2)",
        "",
        "# Underfit (degree 1)",
        "model_under = LinearRegression().fit(X_train, y_train)",
        "y_pred_under = model_under.predict(X_test)",
        "",
        "# Overfit (degree 15)",
        "model_over = make_pipeline(PolynomialFeatures(15), LinearRegression()).fit(X_train, y_train)",
        "y_pred_over = model_over.predict(X_test)",
        "",
        "# Plot",
        "plt.scatter(X_reg, y_reg_true)",
        "plt.plot(X_test, y_pred_under, label='Underfit')",
        "plt.plot(X_test, y_pred_over, label='Overfit')",
        "plt.legend()",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparison / Interpretation",
        "Visualize train vs test error; aim for balance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Cross-Entropy Loss (Log Loss)",
        "",
        "#### Definition",
        "Measures difference between predicted probabilities and true labels.",
        "",
        "#### Why It Is Used in ML / Forecasting",
        "Standard loss for classification; encourages calibrated probabilities.",
        "",
        "#### Pros & Cons",
        "**Pros**: Probabilistic interpretation.  ",
        "**Cons**: Sensitive to extremes.",
        "",
        "#### Equation",
        "\\[ \\text{CE} = - \\sum_{i=1}^{n} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\]",
        "",
        "#### Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual Implementation",
        "def ce_manual(y_true, y_pred):",
        "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)",
        "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))",
        "",
        "ce_val_manual = ce_manual(y_class_true, y_class_pred_prob)",
        "print(f\"Manual CE: {ce_val_manual}\")",
        "",
        "# Keras Implementation",
        "ce_keras = BinaryCrossentropy()",
        "ce_val_keras = ce_keras(y_class_true, y_class_pred_prob).numpy()",
        "print(f\"Keras CE: {ce_val_keras}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparison / Interpretation",
        "Lower is better; used in logistic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 KL Divergence",
        "",
        "#### Definition",
        "Measures how one probability distribution diverges from another.",
        "",
        "#### Why It Is Used in ML / Forecasting",
        "Used in variational autoencoders, GANs for distribution matching.",
        "",
        "#### Pros & Cons",
        "**Pros**: Asymmetric, useful for approximation.  ",
        "**Cons**: Not a true metric (non-symmetric).",
        "",
        "#### Equation",
        "\\[ \\text{KL}(P || Q) = \\sum P(x) \\log \\frac{P(x)}{Q(x)} \\]",
        "",
        "#### Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual Implementation (for discrete)",
        "def kl_manual(p, q):",
        "    p = np.clip(p, 1e-15, 1)",
        "    q = np.clip(q, 1e-15, 1)",
        "    return np.sum(p * np.log(p / q))",
        "",
        "# Example distributions",
        "p = np.array([0.1, 0.4, 0.5])",
        "q = np.array([0.2, 0.3, 0.5])",
        "kl_val_manual = kl_manual(p, q)",
        "print(f\"Manual KL: {kl_val_manual}\")",
        "",
        "# Keras Implementation",
        "def kl_keras(p, q):",
        "    p = K.clip(p, K.epsilon(), 1)",
        "    q = K.clip(q, K.epsilon(), 1)",
        "    return K.sum(p * K.log(p / q), axis=-1)",
        "",
        "kl_val_keras = kl_keras(p, q).numpy()",
        "print(f\"Keras KL: {kl_val_keras}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparison / Interpretation",
        "0 means identical distributions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Other Useful ML Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Huber Loss (robust regression)",
        "",
        "#### Definition",
        "Hybrid of MSE and MAE, less sensitive to outliers.",
        "",
        "#### Why It Is Used in ML / Forecasting",
        "Robust loss for regression with outliers (e.g., financial data).",
        "",
        "#### Pros & Cons",
        "**Pros**: Balances sensitivity.  ",
        "**Cons**: Requires delta tuning.",
        "",
        "#### Equation",
        "\\[ \\text{Huber} = \\begin{cases} \\frac{1}{2}(y - \\hat{y})^2 & |y - \\hat{y}| \\leq \\delta \\\\ \\delta |y - \\hat{y}| - \\frac{1}{2}\\delta^2 & \\text{otherwise} \\end{cases} \\]",
        "",
        "#### Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual Implementation",
        "def huber_manual(y_true, y_pred, delta=1.0):",
        "    error = y_true - y_pred",
        "    is_small = np.abs(error) <= delta",
        "    squared = 0.5 * (error ** 2)",
        "    linear = delta * np.abs(error) - 0.5 * (delta ** 2)",
        "    return np.mean(np.where(is_small, squared, linear))",
        "",
        "huber_val_manual = huber_manual(y_reg_true, y_reg_pred)",
        "print(f\"Manual Huber: {huber_val_manual}\")",
        "",
        "# Keras Implementation",
        "from tensorflow.keras.losses import Huber",
        "huber_keras = Huber()",
        "huber_val_keras = huber_keras(y_reg_true, y_reg_pred).numpy()",
        "print(f\"Keras Huber: {huber_val_keras}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparison / Interpretation",
        "Similar to MAE for large errors, MSE for small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Cosine Similarity (NLP / embeddings)",
        "",
        "#### Definition",
        "Measures angle between vectors, for similarity.",
        "",
        "#### Why It Is Used in ML / Forecasting",
        "In NLP for semantic similarity; embeddings comparison.",
        "",
        "#### Pros & Cons",
        "**Pros**: Magnitude-independent.  ",
        "**Cons**: Ignores magnitude.",
        "",
        "#### Equation",
        "\\[ \\text{Cosine} = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} \\]",
        "",
        "#### Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual Implementation",
        "def cosine_manual(a, b):",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))",
        "",
        "# Example vectors",
        "a = np.array([1, 2, 3])",
        "b = np.array([4, 5, 6])",
        "cos_val_manual = cosine_manual(a, b)",
        "print(f\"Manual Cosine: {cos_val_manual}\")",
        "",
        "# Keras Implementation",
        "def cosine_keras(a, b):",
        "    return K.dot(a, b) / (K.linalg.norm(a) * K.linalg.norm(b))",
        "",
        "cos_val_keras = cosine_keras(tf.constant(a, dtype='float32'), tf.constant(b, dtype='float32')).numpy()",
        "print(f\"Keras Cosine: {cos_val_keras}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparison / Interpretation",
        "1 is identical direction; 0 orthogonal."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}