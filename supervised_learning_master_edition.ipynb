{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61abda97",
   "metadata": {},
   "source": [
    "# Supervised Learning — Master Edition (Single Source of Truth)\n",
    "\n",
    "**Overview:** This notebook is a textbook-grade, self-contained reference for major supervised learning algorithms. It combines deep conceptual theory, mathematical foundations (LaTeX), step-by-step internal mechanics, domain use-cases, visual explanations with Matplotlib, and runnable scikit-learn examples. Use this as a single source of truth for learning, teaching, or interview preparation.\n",
    "\n",
    "---\n",
    "**Contents:**\n",
    "\n",
    "1. Naive Bayes\n",
    "2. Performance Evaluation\n",
    "3. Naive Bayes Optimizations\n",
    "4. K-Nearest Neighbors (KNN)\n",
    "5. Decision Trees\n",
    "6. Linear Regression\n",
    "7. Logistic Regression\n",
    "8. Support Vector Machine (SVM)\n",
    "9. Cross-cutting Topics: Ensembles, Bias-Variance, When to Use What\n",
    "10. Summary Tables & Domain Mapping\n",
    "\n",
    "Run cells sequentially for best results; many visualization cells rely on numpy and matplotlib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a47ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports used throughout the notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log\n",
    "%matplotlib inline\n",
    "print('Common libraries loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a5970",
   "metadata": {},
   "source": [
    "## 1 — Naive Bayes\n",
    "\n",
    "### Conceptual Overview (detailed)\n",
    "\n",
    "Naive Bayes is a probabilistic classification framework built from Bayes' theorem. Historically rooted in statistical decision theory, it became popular in NLP and document classification because it handles high-dimensional sparse inputs efficiently. The **core idea** is to compute the posterior probability of each class given features and pick the class with the highest posterior. The ‘naive’ assumption is that features are conditionally independent given the class — this simplifies the likelihood into a product of one-dimensional likelihoods, dramatically reducing estimation complexity.\n",
    "\n",
    "**Why it exists:** To provide a simple, scalable probabilistic classifier that can be trained with few examples and fast inference. It trades modeling expressiveness for computational efficiency and robustness in high-dimensional sparse settings.\n",
    "\n",
    "### Mathematical foundations\n",
    "\n",
    "Bayes' theorem:\n",
    "\n",
    "$$P(C_k\\mid X) = \\frac{P(X\\mid C_k)P(C_k)}{P(X)}$$\n",
    "\n",
    "Naive independence assumption:\n",
    "\n",
    "$$P(X\\mid C_k)=\\prod_{i=1}^n P(x_i\\mid C_k)$$\n",
    "\n",
    "Often we compute log-posteriors to avoid numerical underflow:\n",
    "\n",
    "$$\\log P(C_k\\mid X) = \\log P(C_k) + \\sum_{i=1}^n \\log P(x_i\\mid C_k) + const.$$ \n",
    "\n",
    "**Variants:** GaussianNB (continuous features), MultinomialNB (counts), BernoulliNB (binary features).\n",
    "\n",
    "### Internal mechanics (step-by-step)\n",
    "\n",
    "1. **Training:** Estimate class priors \\(\\hat P(C_k)\\) and feature likelihood parameters per class (means/variances or counts).\n",
    "2. **Prediction:** For a new sample, compute class log-posteriors using estimated parameters and select argmax.\n",
    "3. **Calibration:** Optionally calibrate probabilities with isotonic or Platt scaling for better probability estimates.\n",
    "\n",
    "### Simple theory example (non-code)\n",
    "\n",
    "Imagine classifying emails as spam/ham. For each word feature, Naive Bayes learns how frequent that word is in spam vs ham. For a new email, multiply (or sum log) the conditioned word probabilities and combine with class priors to get final posterior scores.\n",
    "\n",
    "### Where it's used across ML domains\n",
    "\n",
    "- **NLP / Text classification:** spam detection, sentiment analysis, topic categorization (classic use-case).\n",
    "- **Document filtering and email routing** where interpretability and speed matter.\n",
    "- **Baseline model**: used as a quick baseline against which more complex models are compared.\n",
    "\n",
    "### Strengths & Caveats\n",
    "\n",
    "- **Strengths:** Simple, very fast, works well with sparse, high-dimensional data; requires little memory.\n",
    "- **Caveats:** Conditional independence rarely holds in reality—correlated features can reduce performance. Does not capture feature interactions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd86209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes visual intuition (Gaussian class-conditional densities & posterior)\n",
    "\n",
    "def gaussian_pdf(x, mu, sigma):\n",
    "    return (1/(np.sqrt(2*np.pi)*sigma)) * np.exp(-0.5*((x-mu)/sigma)**2)\n",
    "\n",
    "x = np.linspace(-4, 8, 400)\n",
    "# two class-conditional Gaussians\n",
    "mu0, s0 = 0.5, 1.0\n",
    "mu1, s1 = 3.0, 1.2\n",
    "p0 = gaussian_pdf(x, mu0, s0)\n",
    "p1 = gaussian_pdf(x, mu1, s1)\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(x, p0, label='class 0 likelihood')\n",
    "plt.plot(x, p1, label='class 1 likelihood')\n",
    "plt.title('Naive Bayes — Gaussian class-conditional likelihoods (1D)')\n",
    "plt.xlabel('feature value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Posterior with equal priors\n",
    "prior0 = 0.5\n",
    "prior1 = 0.5\n",
    "post0 = prior0 * p0\n",
    "post1 = prior1 * p1\n",
    "norm = post0 + post1\n",
    "posterior0 = post0 / norm\n",
    "posterior1 = post1 / norm\n",
    "\n",
    "plt.figure(figsize=(8,2.5))\n",
    "plt.plot(x, posterior0, label='P(class0 | x)')\n",
    "plt.plot(x, posterior1, label='P(class1 | x)')\n",
    "plt.title('Posterior probabilities (equal priors)')\n",
    "plt.xlabel('feature value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Observation: decision boundary where posteriors cross.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c72e096",
   "metadata": {},
   "source": [
    "## 2 — Performance Evaluation (Detailed Theory & Use)\n",
    "\n",
    "### Conceptual overview\n",
    "\n",
    "Performance evaluation answers: *How good is my model?* and *where does it fail?* Selecting metrics depends on the problem (classification vs regression) and business priorities (e.g., false negatives more costly than false positives in medical tests).\n",
    "\n",
    "### Core concepts & metrics\n",
    "\n",
    "- **Confusion matrix** components: TP, FP, TN, FN.\n",
    "- **Accuracy**: overall correctness but can be misleading on imbalanced data.\n",
    "- **Precision & Recall**: precision measures correctness of positive predictions; recall measures coverage of actual positives.\n",
    "- **F1-score** balances precision and recall.\n",
    "- **ROC-AUC** measures separability across thresholds. **PR-AUC** is more informative for highly imbalanced problems.\n",
    "\n",
    "### Cross-validation and model selection\n",
    "\n",
    "- **k-fold CV** provides robust performance estimates by averaging results across folds.\n",
    "- **Nested CV** is important for honest hyperparameter tuning to avoid optimistic bias.\n",
    "\n",
    "### Use across ML domains\n",
    "\n",
    "- In healthcare, **recall** (sensitivity) is prioritized to avoid missing positive cases.\n",
    "- In spam filtering, **precision** matters to avoid false positives (legitimate emails marked spam).\n",
    "- In recommender systems, business-derived metrics (CTR, revenue uplift) often matter more than raw accuracy.\n",
    "\n",
    "### Practical tips\n",
    "\n",
    "- Choose metrics aligned with costs of different error types.\n",
    "- Use stratified CV for classification to preserve class ratios.\n",
    "- Report multiple metrics (accuracy + precision/recall + calibration) for transparency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc03a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance visuals: ROC and Precision-Recall\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=3, weights=[0.7], flip_y=0.01, random_state=2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "clf = LogisticRegression(max_iter=500)\n",
    "clf.fit(X_train, y_train)\n",
    "probs = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(fpr, tpr, label=f'AUC={roc_auc:.3f}')\n",
    "plt.plot([0,1],[0,1],'--', color='gray')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, probs)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision)\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3435d1c",
   "metadata": {},
   "source": [
    "## 3 — Naive Bayes Optimizations (Detailed Theory)\n",
    "\n",
    "### Why optimize?\n",
    "Naive Bayes is simple and effective, but several practical adjustments improve robustness, especially in NLP: smoothing, feature selection, and more realistic likelihood models.\n",
    "\n",
    "### Laplace (additive) smoothing\n",
    "Smoothing avoids zero probabilities for unseen tokens. With vocabulary size |V| and smoothing parameter \\(\\alpha\\):\n",
    "\n",
    "$$P(w\\mid C)=\\frac{count(w,C)+\\alpha}{\\sum_{w'}count(w',C)+\\alpha|V|}$$\n",
    "\n",
    "Smaller \\(\\alpha\\) (e.g., 0.1) is less aggressive; \\(\\alpha=1\\) is Laplace smoothing.\n",
    "\n",
    "### Feature selection and weighting\n",
    "- **TF-IDF** downweights very frequent words that are less informative.\n",
    "- **Chi-square, mutual information** select tokens most correlated with labels.\n",
    "\n",
    "### Calibration & hybrid models\n",
    "- Naive Bayes probabilities can be poorly calibrated; isotonic regression or Platt scaling can help.\n",
    "- Hybrid models stack Naive Bayes as features into stronger learners (e.g., NB + Logistic Regression).\n",
    "\n",
    "### Use-cases across ML\n",
    "- As a competitive baseline in text tasks.\n",
    "- Fast initial model when building large-scale pipelines (e.g., email triage).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec910b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate smoothing effect: token probabilities with different alpha\n",
    "from collections import Counter\n",
    "\n",
    "spam_docs = ['buy cheap meds now', 'cheap meds available', 'limited offer buy']\n",
    "ham_docs = ['project meeting schedule', 'discuss project', 'let us meet tomorrow']\n",
    "all_docs = spam_docs + ham_docs\n",
    "vocab = list(set(' '.join(all_docs).split()))\n",
    "V = len(vocab)\n",
    "\n",
    "def token_prob(token, docs, alpha):\n",
    "    counts = Counter(' '.join(docs).split())\n",
    "    total = sum(counts.values())\n",
    "    return (counts[token] + alpha) / (total + alpha * V)\n",
    "\n",
    "print('Vocab sample:', vocab[:8])\n",
    "for alpha in [0.0, 0.5, 1.0]:\n",
    "    probs = {t: token_prob(t, spam_docs, alpha) for t in vocab}\n",
    "    print(f'alpha={alpha}, sample probs:', list(probs.items())[:6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666896ca",
   "metadata": {},
   "source": [
    "## 4 — K-Nearest Neighbors (KNN)\n",
    "\n",
    "### Conceptual overview\n",
    "KNN is an instance-based (lazy) algorithm: it does not build a global model during training. Instead, it stores training samples and at prediction time finds the k closest training points according to a distance metric. The predicted label is typically the majority among neighbors (classification) or the average (regression).\n",
    "\n",
    "### When to use\n",
    "- Small to medium datasets where training time should be minimal and interpretability is helpful.\n",
    "- Problems where similarity intuition is strong (e.g., recommendation by nearest users/items).\n",
    "\n",
    "### Limitations and scalability\n",
    "- Prediction cost is O(n) per query; high-dimensional data degrades distance usefulness (curse of dimensionality).\n",
    "- Preprocessing: feature scaling is essential.\n",
    "\n",
    "### Simple (non-code) example\n",
    "Classify a new flower by looking at the closest k labeled flowers in the feature space (petal length, petal width, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122e4089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN decision boundary visualization for varying k\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(n_samples=300, n_features=2, n_informative=2, n_redundant=0, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "def plot_knn(k):\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(X_train, y_train)\n",
    "    xx, yy = np.meshgrid(np.linspace(X[:,0].min()-1,X[:,0].max()+1,200), np.linspace(X[:,1].min()-1,X[:,1].max()+1,200))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "    plt.scatter(X[:,0], X[:,1], c=y, s=15)\n",
    "    plt.title(f'KNN decision boundary (k={k})')\n",
    "    plt.show()\n",
    "\n",
    "for k in [1,3,9]:\n",
    "    plot_knn(k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abeece6",
   "metadata": {},
   "source": [
    "## 5 — Decision Trees\n",
    "\n",
    "### Conceptual overview\n",
    "Decision Trees recursively partition the feature space into regions with mostly-homogeneous labels. They are non-parametric and create human-readable rules (if-then statements). The splitting criterion (e.g., information gain, Gini impurity) measures which split reduces uncertainty the most.\n",
    "\n",
    "### Why they are useful\n",
    "- Interpretability: rules are easy to visualize and explain.\n",
    "- Handle mixed data types (categorical + numeric) without heavy preprocessing.\n",
    "\n",
    "### Where they can fail\n",
    "- Trees can overfit: a deep tree can memorize noise. Regularization (max depth, min samples) and ensembles mitigate this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b58f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy plot and decision tree example\n",
    "ps = np.linspace(0.001, 0.999, 500)\n",
    "entropy = - (ps * np.log2(ps) + (1-ps) * np.log2(1-ps))\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(ps, entropy)\n",
    "plt.title('Binary entropy H(p)')\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('Entropy')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "X, y = make_moons(n_samples=200, noise=0.25, random_state=0)\n",
    "clf = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
    "clf.fit(X, y)\n",
    "plt.figure(figsize=(6,4))\n",
    "plot_tree(clf, filled=True)\n",
    "plt.title('Decision Tree (make_moons)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1c0011",
   "metadata": {},
   "source": [
    "## 6 — Linear Regression\n",
    "\n",
    "### Conceptual overview\n",
    "Linear Regression models the expectation of the target as a linear combination of features. It's the simplest parametric regression model and serves as the foundation for many statistical and machine learning techniques. It provides interpretable coefficients indicating marginal effects.\n",
    "\n",
    "### Derivation (OLS normal equation)\n",
    "Given matrix X (with column of ones for intercept) and target y, OLS solves:\n",
    "\n",
    "$$\\hat{\\beta} = \\arg\\min_\\beta ||y - X\\beta||_2^2$$\n",
    "\n",
    "Setting derivative to zero yields the normal equation:\n",
    "\n",
    "$$X^TX\\hat{\\beta} = X^Ty \\Rightarrow \\hat{\\beta} = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "### Use-cases and notes\n",
    "- Used for trend estimation, baseline regressors, and interpretation.\n",
    "- Use regularized variants (Ridge, Lasso) when multicollinearity or overfitting occurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d3d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression example with synthetic noisy data\n",
    "np.random.seed(0)\n",
    "X = np.linspace(0, 10, 50)\n",
    "y = 3.0 * X + 4.0 + np.random.normal(scale=5.0, size=X.shape)\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "X_mat = X.reshape(-1,1)\n",
    "reg = LinearRegression().fit(X_mat, y)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(X, y, label='data')\n",
    "plt.plot(X, reg.predict(X_mat), color='red', label='OLS fit')\n",
    "plt.title('Linear Regression fit (noisy data)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print('OLS coef, intercept:', reg.coef_, reg.intercept_)\n",
    "\n",
    "# Ridge example to illustrate regularization effect\n",
    "ridge = Ridge(alpha=10.0).fit(X_mat, y)\n",
    "print('Ridge coef, intercept:', ridge.coef_, ridge.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252e1c9",
   "metadata": {},
   "source": [
    "## 7 — Logistic Regression\n",
    "\n",
    "### Conceptual overview\n",
    "Logistic Regression models the log-odds of the positive class as a linear function of inputs and maps that to a probability via the sigmoid function. It's a discriminative model (models P(y|x) directly) and widely used for binary classification due to simplicity and well-behaved convex loss.\n",
    "\n",
    "### Loss and optimization\n",
    "The negative log-likelihood (log-loss) is convex; common solvers use Newton's method variants or gradient-based solvers to find parameters.\n",
    "\n",
    "### Use cases\n",
    "- Binary classification tasks across domains (medical diagnosis, credit scoring, click-through prediction) where interpretability and probability estimates matter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69507ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid plot and logistic regression decision boundary\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "z = np.linspace(-10, 10, 400)\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(z, sigmoid(z)); plt.title('Sigmoid function'); plt.grid(True); plt.show()\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_informative=2, n_redundant=0, random_state=1)\n",
    "clf = LogisticRegression().fit(X, y)\n",
    "xx, yy = np.meshgrid(np.linspace(X[:,0].min()-1, X[:,0].max()+1, 200), np.linspace(X[:,1].min()-1, X[:,1].max()+1, 200))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.figure(figsize=(6,4)); plt.contourf(xx, yy, Z, alpha=0.3); plt.scatter(X[:,0], X[:,1], c=y); plt.title('Logistic Regression boundary'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eae1a3",
   "metadata": {},
   "source": [
    "## 8 — Support Vector Machine (SVM)\n",
    "\n",
    "### Conceptual overview\n",
    "SVM searches for a hyperplane that maximizes the margin between classes. Conceptually, maximum margin improves generalization because it finds the most robust separator.\n",
    "\n",
    "### Geometry & optimization\n",
    "For linearly separable data (hard-margin):\n",
    "\n",
    "$$\\min_{w,b} \\frac{1}{2}||w||^2 \\quad \\text{s.t. } y_i(w\\cdot x_i + b) \\ge 1$$\n",
    "\n",
    "Margin width = \\(2/||w||\\). For non-separable data introduce slack variables and regularization parameter C.\n",
    "\n",
    "### Kernels and non-linear separation\n",
    "Kernels implicitly map data to higher-dimensional spaces where a linear separator may exist; common kernels include RBF and polynomial.\n",
    "\n",
    "### Use-cases\n",
    "- Effective in high-dimensional tasks such as text categorization and early computer vision tasks. SVMs were heavily used before deep learning became dominant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cc3547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM margin visualization\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=120, centers=2, random_state=6)\n",
    "clf = SVC(kernel='linear', C=1.0).fit(X, y)\n",
    "plt.figure(figsize=(6,4))\n",
    "xx = np.linspace(X[:,0].min()-1, X[:,0].max()+1, 200)\n",
    "yy = np.linspace(X[:,1].min()-1, X[:,1].max()+1, 200)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Z = clf.predict(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape)\n",
    "plt.contourf(XX, YY, Z, alpha=0.3)\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "sv = clf.support_vectors_\n",
    "plt.scatter(sv[:,0], sv[:,1], s=100, facecolors='none', edgecolors='k', label='support vectors')\n",
    "plt.title('Linear SVM with support vectors (circled)')\n",
    "plt.legend(); plt.show()\n",
    "print('Support vectors count per class:', clf.n_support_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7b13c",
   "metadata": {},
   "source": [
    "## 9 — Cross-cutting Topics: Ensembles, Bias-Variance, When to Use What\n",
    "\n",
    "### Bias-Variance tradeoff (intuition)\n",
    "- **Bias**: error from wrong model assumptions (underfitting). High bias models are too simple.\n",
    "- **Variance**: error from sensitivity to small fluctuations in training set (overfitting). High variance models are too complex.\n",
    "A good model balances bias and variance; regularization and ensembling are ways to control variance.\n",
    "\n",
    "### Ensembles (brief)\n",
    "- **Bagging (e.g., Random Forests):** reduces variance by averaging many trees trained on bootstrap samples.\n",
    "- **Boosting (e.g., AdaBoost, Gradient Boosting):** sequentially focuses on hard-to-predict examples to reduce bias and produce strong learners.\n",
    "\n",
    "### When to use what (practical rules)\n",
    "- **Naive Bayes:** fast baseline for text/NLP.\n",
    "- **Logistic Regression:** when you need interpretable probability estimates.\n",
    "- **Decision Trees / Random Forests:** when interpretability and handling mixed feature types matter.\n",
    "- **SVM:** small-to-medium high-dimensional problems with clear margins.\n",
    "- **KNN:** small datasets where similarity is meaningful.\n",
    "- **Linear Regression:** baseline for regression; use Ridge/Lasso for regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9cd818",
   "metadata": {},
   "source": [
    "## 10 — Summary Tables & Domain Mapping\n",
    "\n",
    "| Algorithm | Core Idea | Typical Domains | Strength | Limitation |\n",
    "|---|---|---|---|---|\n",
    "| Naive Bayes | Probabilistic with independence assumption | NLP, document classification | Fast, simple, works with high-dim sparse data | Independence assumption may be unrealistic |\n",
    "| KNN | Similarity-based voting | Recommendation, small classification tasks | Simple, non-parametric | Slow at prediction, needs scaling |\n",
    "| Decision Tree | Recursive partitioning | Finance, healthcare (rule-based models) | Interpretable, handles mixed data | Prone to overfit |\n",
    "| Linear Regression | Linear modeling of target | Economics, trend analysis | Interpretable coefficients | Fails with non-linearity |\n",
    "| Logistic Regression | Linear model for log-odds | Healthcare, marketing | Probabilistic outputs, interpretable | Linear decision boundary |\n",
    "| SVM | Max-margin classifier | Bioinformatics, vision (historically) | Effective in high-dim spaces | Slow on large datasets |\n",
    "\n",
    "---\n",
    "\n",
    "### Final notes\n",
    "This master notebook aims to be both a learning document and a quick reference. For deeper theory consult Bishop (PRML) or Hastie et al. (ESL).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790da591",
   "metadata": {},
   "source": [
    "## References & Further Reading\n",
    "\n",
    "- Christopher Bishop — *Pattern Recognition and Machine Learning*\n",
    "- Hastie, Tibshirani, Friedman — *The Elements of Statistical Learning*\n",
    "- Scikit-learn documentation: https://scikit-learn.org\n",
    "- Goodfellow, Bengio, Courville — *Deep Learning*\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
